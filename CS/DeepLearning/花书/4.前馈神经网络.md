## 前馈神经网络

### 1. 整流线性单元

[参考博客](https://blog.csdn.net/stu_shanghui/article/details/94492281)

- relu
- leaky relu
- Prelu
- elu

![](https://img-blog.csdn.net/20161120172223507)

- Crelu
- Rrelu

- maxout: 
  - maxout 单元将 z 划分为每组具有 k 个值的组
  - 因为每个单元由多个过滤器驱动， maxout 单元具有一些冗余来帮助它们抵抗一种被称为 灾难遗忘（ catastrophic forgetting）的现象
- logistic sigmoid
  - sigmoid 单元在其大部分定义域内都饱和——当 z 取绝对值
    很大的正值时，它们饱和到一个高值，当 z 取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当 z 接近 0 时它们才对输入强烈敏感
  - sigmoid 单元的广泛饱和性会使得基于梯度的学习变得非常困难
- 双曲正切函数:
  - 当必须要使用 sigmoid 激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好



其他

- 径向基函数（ radial basis function, RBF）
- softplus函数
- 硬双曲正切函数（ hard tanh）



### 2. 架构设计

- 主要的架构考虑是选择网络的深度和每一层的宽度
- 更深层的网络通常能够
  对每一层使用更少的单元数和更少的参数，并且经常容易泛化到测试集，但是通常也更难以优化

#### 万能近似定理

一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 ‘‘挤压’’ 性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的 Borel 可测函数

**万能近似定理说明了，存在一个足够大的网络能够达到我们所希望的任意精度，但是定理并没有说这个网络有多大**



### 3. 反向传播和其他的微分算法

#### 3.1 计算图

- 图中的每一个节点来表示一个变量。变量可以是标量、向量、矩
  阵、张量、或者甚至是另一类型的变量
- 操作是指一个或多个变量的简单函数





