## 2. 概率和信息论

**概率分布**（ probability distribution）用来描述随机变量或一簇随机变量在每一
个可能取到的状态的可能性大小。



### 2.1常用函数的有用性质

#### logistic sigmoid 函数

$$
\sigma (x) = \frac{1}{ 1 + exp(−x)}
$$

- logistic sigmoid 函数通常用来产生 Bernoulli 分布中的参数 ϕ，因为它的范围是(0, 1)
- sigmoid 函数在变量取绝对值非常大的正值或负值时会出现 饱和（ saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感



#### softplus 函数

$$
\zeta(x) = log(1 + exp(x))
$$

softplus 函数可以用来产生正态分布的 β 和 σ 参数，因为它的范围是 (0, 1)



#### 2.2 信息论

信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事
件发生，能提供更多的信息

通过这种基本想法来量化信息：

- 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
-  较不可能发生的事件具有更高的信息量。
-  独立事件应具有增量的信息

#### 自信息：（ self-information）

$$
I(x) = − log P(x)
$$

自信息只处理单个的输出



#### 香农熵（ Shannon entropy）

来对整个概率分布中的不确定性总量进行量化
$$
H(x) = E_{x∼P}[I(x)] = −E_{x∼P}[log P(x)]
$$

- 也记作 H(P)。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量
- 当 x 是连续的， 香农熵被称为 微分熵（ differential entropy）



#### KL散度

如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可
以使用 KL 散度（ Kullback-Leibler (KL) divergence）来衡量这两个分布的差异
$$
D_{KL}(P||Q) = E_{x∼P} [log \frac{P(x)}{Q(x)}] = E_{x∼P}[log P(x) − log Q(x)]:
$$

- 散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离
- 它并不是真的距离因为它不是对称的：对于某些 P 和 Q

$$
D_{KL}(P||Q) \neq D_{KL}(Q||P)
$$

- 一个和 KL 散度密切联系的量是 交叉熵（ cross-entropy） 

$$
H(P, Q) = H(P) +D_{KL}(P||Q)
\\
= −E_{x∼P} log Q(x)
$$



#### 结构化概率模型

把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表示概率分布

**用图来表示这种概率分布的分解**

- 有向模型
- 无向模型