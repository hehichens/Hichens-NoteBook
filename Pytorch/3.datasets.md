# Datasets

## 1. Image Dataset

```python
class ImageDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.annotations = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform
        
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, index):
        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0]) # first column is the image name
        image = io.imread(img_path) # from skimage import io
        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))
        
        if self.transform:
            image = self.transform(image)
            
         return (image, y_label)
    
# examples
dataset = ImageDataset(
	csv_file = csv_file_name,
    root_dir = root_dir_name,
    transform=transform.ToTensor(), # import torchvision.transforms as transforms
)
train_set, test_set = torch.utils.data.random_split(dataset, [5, 5])
# from torch.utils.data import DataLoader
train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)
```

## Text Dataset

[link](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/custom_dataset_txt/loader_customtext.py)

for some reson i cannot configuration enviroment



## Data Augmentation

```python
# from torchvision import transforms
my_transforms = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resized(size),
    transforms.RandomCrop(size),
    transforms.ColorJitter(brightness),
    transforms.RandomRotation(degrees),
    transforms.RandomHorizontalFlip(p),
    transforms.RandomVerticalFlip(p),
    transforms.RandomGrayScale(p),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
])
```



## Tips

### common mistakes

1. overfit single batch
2. set training as model.train() and set evaluate as model.eval()
3. pay attention to .zero_grad()
4. do not combine CrossEntropy and softmax 
5. set bias=Flase when use batchnorm
6. clipping gradients when use RNN, GRU, LSTM # nn.utils.clip_grad_norm(model.parameters, max_norm=1 )

### Progress Bar

```python
from tqdm import tqdm
loop = tqdm(enumetate(data_loader), total=len(data_loader), leave=False) # leave=False, one bar
for batch_idx, (data, targets) in loop:
    ...
    loop.set_description(f"Epoch [{epoch}/{num_epochs}]")
    loop.set_postfix(loss=, acc=)
    
```



### Reproducible result

```python
seed = 42
torch.manul_seed(seed)
np.random.seed(seed)
random.seed(seed)

# if using cuda
torch.cuda.manul_seed(seed)
torch.backends.cudnn.determinstic = True
torch.backends.cudnn.benchmark = False

```



### Caculate Mean and Standard Deviation of Data

```python
# from data_loader
def get_mean_std(loader):
    channels_sum, channels_squared_sum, num_batches = 0, 0, 0
    for data, _ in loader:
        channels_sum += torch.mean(data, dim=[0, 2, 3])
        channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])
        num_batches += 1
    mean = channels_sum / num_batches
    std = (channels_squared_sum / num_batches - mean**2)**0.5
    return mean, std
        
```



### Weight Initialization

```python
class NN(nn.Module):
    def __init__():
        ...
        self.initialize_weights()
    def initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_uniform_(m.weight)

                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

            elif isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
```

