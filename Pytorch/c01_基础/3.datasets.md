# Datasets

## 1. Image Dataset

```python
class ImageDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.annotations = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform
        
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, index):
        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0]) # first column is the image name
        image = io.imread(img_path) # from skimage import io
        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))
        
        if self.transform:
            image = self.transform(image)
            
         return (image, y_label)
    
# examples
dataset = ImageDataset(
	csv_file = csv_file_name,
    root_dir = root_dir_name,
    transform=transform.ToTensor(), # import torchvision.transforms as transforms
)
train_set, test_set = torch.utils.data.random_split(dataset, [5, 5])
# from torch.utils.data import DataLoader
train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)
```

## 2. Text Dataset

[link](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/Basics/custom_dataset_txt/loader_customtext.py)

```python
import torch
import torchvision
import spacy; spacy_eng = spacy.load('en_core_web_sm')
import pandas as pd
import os 

from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader, Dataset
from torchvision.transforms import transforms


class Vocabulary:
    def __init__(self, freq_threshod=5):
        self.idx2str = {0: "<PAD>", 1: "<SOS>", 2: "<EOS>", 3: "<UNK>"}
        self.str2idx = {v:k for k, v in self.idx2str.items()}
        self.freq_threshod = freq_threshod

    def __len__(self):
        return len(self.idx2str)

    @staticmethod
    def tokenizer_eng(text):
        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]

    def build_vocabulary(self, sentence_list):
        frequencies = {}
        idx = 4

        for sentence in sentence_list:
            for word in self.tokenizer_eng(sentence):
                if word not in frequencies:
                    frequencies[word] = 1
                else:
                    frequencies[word] += 1
                if frequencies[word] == self.freq_threshod:
                    self.str2idx[word] = idx
                    self.idx2str[idx] = word
                    idx += 1

    def numericalize(self, text):
        tokenized_text = self.tokenizer_eng(text)
        return [
            self.str2idx[token] if token in self.str2idx else self.str2idx['<UNK>']
            for token in tokenized_text
        ]


class TextDataset(Dataset):
    def __init__(self, text_file, freq_threshod=5):
        self.df = pd.read_table(text_file)
        self.text = self.df.iloc[:, 1].tolist()

        self.vocab = Vocabulary()
        self.vocab.build_vocabulary(self.text)

    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        text = self.text[index]
        num_text = [self.vocab.str2idx['<SOS>']]
        num_text += self.vocab.numericalize(text)
        num_text.append(self.vocab.str2idx['<EOS>'])
        # print(torch.tensor(num_text))
        return torch.tensor(num_text)


class MyCollate:
    def __init__(self, pad_idx):
        self.pad_idx = pad_idx

    def __call__(self, batch):
        targets = [item[1] for item in batch]
        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)
        return targets


def main():
    text_file = "/home/hichens/Practice/torch/Datasets/Flickr8k_text/Flickr8k.token.txt"
    dataset = TextDataset(text_file)
    batch_size = 32
    num_works = 8
    pad_idx = dataset.vocab.str2idx["<PAD>"]
    loader = DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        num_workers=num_works,
        shuffle=True,
        pin_memory=True,
        # collate_fn=MyCollate(pad_idx=pad_idx)
    )
    for text in loader:
        print(text.shape)
        break

def test():
    text_file = "/home/hichens/Practice/torch/Datasets/Flickr8k_text/Flickr8k.token.txt"
    dataset = TextDataset(text_file)
    dataset.__getitem__(9751)

if __name__ == "__main__":
    # test()
    main()
```





## 3. Data Augmentation

```python
# from torchvision import transforms
my_transforms = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resized(size),
    transforms.RandomCrop(size),
    transforms.ColorJitter(brightness),
    transforms.RandomRotation(degrees),
    transforms.RandomHorizontalFlip(p),
    transforms.RandomVerticalFlip(p),
    transforms.RandomGrayScale(p),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
])
```



## torchtext

```python
from torchtext.data import Field, BucketIterator, TabularDataset

Feild(
	sequential=bool,
    use_vocab=bool,
    tokenize,
    lower=bool,
    
)
# method:
	build_vocab(train_data, max_size, min_freq, vectors)

    
train_data, test_data = TabularDataset.splits(
    path,
    train,
    test,
    format, 
    fields,
)


train_iter, test_iter = BucketIterator.splits = (
    (train_data, test_data),
    batch_size,
    device,
)
```

